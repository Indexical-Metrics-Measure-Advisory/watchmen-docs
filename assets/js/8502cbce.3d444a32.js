"use strict";(globalThis.webpackChunkwatchmen_docs=globalThis.webpackChunkwatchmen_docs||[]).push([[50987],{28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(96540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},50085:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"appendix/how-to-maintain-model","title":"Model Maintenance Process","description":"This document outlines the standard workflow for maintaining data models in Watchmen, aligned with the Data Development Process. It details the steps for handling model changes, additions, and new system integrations using standard Watchmen modules.","source":"@site/versioned_docs/version-18.0.0/999-appendix/how_to_maintain_model.md","sourceDirName":"999-appendix","slug":"/appendix/how-to-maintain-model","permalink":"/docs/18.0.0/appendix/how-to-maintain-model","draft":false,"unlisted":false,"editUrl":"https://github.com/Indexical-Metrics-Measure-Advisory/watchmen-docs/tree/main/versioned_docs/version-18.0.0/999-appendix/how_to_maintain_model.md","tags":[],"version":"18.0.0","lastUpdatedBy":"luke0623","lastUpdatedAt":1770298247000,"sidebarPosition":2,"frontMatter":{"id":"how-to-maintain-model","title":"Model Maintenance Process","sidebar_position":2},"sidebar":"sidebar","previous":{"title":"License","permalink":"/docs/18.0.0/appendix/license"},"next":{"title":"compare","permalink":"/docs/18.0.0/escosystem/compare"}}');var t=s(74848),r=s(28453);const o={id:"how-to-maintain-model",title:"Model Maintenance Process",sidebar_position:2},l="Model Maintenance Process",d={},a=[{value:"1. Changes to Existing Models",id:"1-changes-to-existing-models",level:2},{value:"2. Adding New Models",id:"2-adding-new-models",level:2},{value:"3. Onboarding New Systems",id:"3-onboarding-new-systems",level:2},{value:"4. Enterprise Governance &amp; Engineering Process",id:"4-enterprise-governance--engineering-process",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"model-maintenance-process",children:"Model Maintenance Process"})}),"\n",(0,t.jsxs)(n.p,{children:["This document outlines the standard workflow for maintaining data models in Watchmen, aligned with the ",(0,t.jsx)(n.a,{href:"/docs/18.0.0/getting-started/data-development",children:"Data Development Process"}),". It details the steps for handling model changes, additions, and new system integrations using standard Watchmen modules."]}),"\n",(0,t.jsx)(n.h2,{id:"1-changes-to-existing-models",children:"1. Changes to Existing Models"}),"\n",(0,t.jsx)(n.p,{children:"When the upstream data structure changes (e.g., adding columns, modifying data types):"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configure Ingestion"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Navigate to the ",(0,t.jsx)(n.code,{children:"Ingestion"})," workbench."]}),"\n",(0,t.jsx)(n.li,{children:"Update the specific table configuration to capture the new schema changes (DDL) from the source system."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Topics"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Go to ",(0,t.jsx)(n.code,{children:"Admin -> Topic"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Select the target ",(0,t.jsx)(n.strong,{children:"Raw Topic"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Update the ",(0,t.jsx)(n.strong,{children:"Factors"})," to reflect the new columns or data types."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Orchestrate Pipelines"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Go to ",(0,t.jsx)(n.code,{children:"Admin -> Pipeline"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Review pipelines triggered by the updated topic."}),"\n",(0,t.jsxs)(n.li,{children:["Modify transformation units (e.g., ",(0,t.jsx)(n.code,{children:"Insert Row"}),", ",(0,t.jsx)(n.code,{children:"Merge Row"}),") to map the new factors to downstream ",(0,t.jsx)(n.strong,{children:"Business Topics"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validate & Profile"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Check ",(0,t.jsx)(n.code,{children:"Admin -> Monitor Logs"})," to verify that the pipeline processes the new data structure without errors."]}),"\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.code,{children:"Topic Profile"})," to inspect the distribution of values in the new factors."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"2-adding-new-models",children:"2. Adding New Models"}),"\n",(0,t.jsx)(n.p,{children:"When business requirements dictate the addition of new tables or entities:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configure Ingestion"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["In the ",(0,t.jsx)(n.code,{children:"Ingestion"})," workbench, add the new table to the model configuration."]}),"\n",(0,t.jsx)(n.li,{children:"Set up extraction keys (Primary Key) and incremental triggers (Update Time)."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Topics"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Define a new ",(0,t.jsx)(n.strong,{children:"Raw Topic"})," to map the ingested table structure one-to-one."]}),"\n",(0,t.jsxs)(n.li,{children:["Define a target ",(0,t.jsx)(n.strong,{children:"Business Topic"})," for the finalized analytical model."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Orchestrate Pipelines"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Create a new ",(0,t.jsx)(n.strong,{children:"Pipeline"})," associated with the Raw Topic."]}),"\n",(0,t.jsx)(n.li,{children:"Define stages to cleanse, transform, and load data into the Business Topic."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Assure Data Quality"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Go to ",(0,t.jsx)(n.code,{children:"DQC -> Monitor Rules"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Configure validation rules (e.g., Null Check, Value Range) for the new topics to ensure data integrity from day one."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"3-onboarding-new-systems",children:"3. Onboarding New Systems"}),"\n",(0,t.jsx)(n.p,{children:"When integrating a completely new external system:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Register Data Sources"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Go to ",(0,t.jsx)(n.code,{children:"Admin -> Data Source"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"Create a new connection profile for the external system."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configure Ingestion"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a new ingestion group and import the system's table structures."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Strategic Modeling"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Direct Mapping"}),": If the source system is already standardized (e.g., an aggregated view), map it directly to Business Topics to minimize latency."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration Layer"}),": For legacy or complex systems, use Raw Topics as a staging layer and leverage ",(0,t.jsx)(n.strong,{children:"Pipelines"}),' for "Heterogeneous Model Transformation".']}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operate & Automate"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.code,{children:"Admin -> Toolbox"})," to trigger initial data snapshots or historical backfills."]}),"\n",(0,t.jsxs)(n.li,{children:["Verify the end-to-end flow using ",(0,t.jsx)(n.code,{children:"Consanguinity"})," to visualize the new lineage."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"4-enterprise-governance--engineering-process",children:"4. Enterprise Governance & Engineering Process"}),"\n",(0,t.jsx)(n.p,{children:"For organizations with complex data landscapes or strict compliance requirements, it is highly recommended to wrap the above technical steps within a formal engineering management framework."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Change Approval Workflow"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Implement a ",(0,t.jsx)(n.strong,{children:"Change Approval Board (CAB)"})," or a lightweight review process before modifying core Business Topics."]}),"\n",(0,t.jsx)(n.li,{children:"Ensure that any schema change (e.g., modifying a Factor type) is reviewed for downstream impact on Reports and Indicators."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Role-Based Access Control (RBAC)"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Strictly separate duties: ",(0,t.jsx)(n.strong,{children:"Data Developers"})," configure Topics and Pipelines in the Development Environment. ",(0,t.jsx)(n.strong,{children:"Data Stewards"})," or ",(0,t.jsx)(n.strong,{children:"Admins"})," review and deploy changes to Production."]}),"\n",(0,t.jsxs)(n.li,{children:["Use Watchmen's ",(0,t.jsx)(n.code,{children:"User Group"})," and ",(0,t.jsx)(n.code,{children:"Space"})," features to isolate sensitive data domains."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environment Promotion Strategy"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Adopt a standard ",(0,t.jsx)(n.strong,{children:"Dev -> Test -> Prod"})," lifecycle."]}),"\n",(0,t.jsxs)(n.li,{children:["Develop and test model changes in a lower environment first. Use Watchmen's ",(0,t.jsx)(n.strong,{children:"Toolbox"})," to export/import definitions between environments, ensuring consistency."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation as Code"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Treat data models as first-class citizens. Maintain a changelog for Topic definitions and Pipeline logic."}),"\n",(0,t.jsx)(n.li,{children:'Enforce the "Description" field for every new Topic and Factor to ensure the Data Catalog remains self-documenting for AI and human users.'}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);